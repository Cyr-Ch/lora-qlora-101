{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab LoRA Fine-Tuning Tool\n",
        "\n",
        "This notebook fine-tunes open-source chat models (e.g., `meta-llama/Llama-2-7b-chat-hf`, `mistralai/Mistral-7B-Instruct`) using standard LoRA adapters without 4-bit quantization. It targets users who have at least a single 16-24 GB GPU available in Google Colab and prefer a simpler, full-precision setup compared to QLoRA.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š What is LoRA? (Low-Rank Adaptation)\n",
        "\n",
        "### The Problem: Fine-Tuning Large Language Models is Expensive\n",
        "\n",
        "Traditional fine-tuning updates **all** the parameters of a pre-trained model. For a 7B parameter model like Llama-2:\n",
        "- **Memory**: You need to store the model weights (~14 GB in fp16), gradients (~14 GB), and optimizer states (~28 GB for Adam). That's **~56 GB** just for training!\n",
        "- **Storage**: Each fine-tuned model is a full copy (~14 GB per task).\n",
        "- **Catastrophic forgetting**: Full fine-tuning can overwrite the model's pre-trained knowledge.\n",
        "\n",
        "### The Solution: LoRA (Low-Rank Adaptation)\n",
        "\n",
        "**LoRA** was introduced in the paper *\"LoRA: Low-Rank Adaptation of Large Language Models\"* (Hu et al., 2021). The key insight is:\n",
        "\n",
        "> **The weight updates during fine-tuning have low intrinsic rank.**\n",
        "\n",
        "This means we don't need to update all parameters, we can approximate the updates with much smaller matrices!\n",
        "\n",
        "### ğŸ§® The Mathematics Behind LoRA\n",
        "\n",
        "For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, instead of computing:\n",
        "\n",
        "$$W = W_0 + \\Delta W$$\n",
        "\n",
        "where $\\Delta W \\in \\mathbb{R}^{d \\times k}$ has $d \\times k$ trainable parameters, LoRA decomposes the update as:\n",
        "\n",
        "$$W = W_0 + BA$$\n",
        "\n",
        "where:\n",
        "- $B \\in \\mathbb{R}^{d \\times r}$ (down-projection)\n",
        "- $A \\in \\mathbb{R}^{r \\times k}$ (up-projection)\n",
        "- $r \\ll \\min(d, k)$ is the **rank** (typically 8-64)\n",
        "\n",
        "**Parameter reduction example:**\n",
        "- Original: $d \\times k = 4096 \\times 4096 = 16.7M$ parameters\n",
        "- LoRA with $r=16$: $(d \\times r) + (r \\times k) = 4096 \\times 16 + 16 \\times 4096 = 131K$ parameters\n",
        "- **Reduction: 99.2%!**\n",
        "\n",
        "### ğŸ”§ How LoRA Works During Training\n",
        "\n",
        "```\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    Input x â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Wâ‚€ (frozen)    â”‚â”€â”€â”€â”€â”€â”€â”\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
        "                                             â–¼\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”\n",
        "    Input x â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  A  â”‚â”€â”€â–ºâ”‚  B  â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚ + â”‚â”€â”€â”€â”€â–º Output\n",
        "                    â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”˜\n",
        "                    (rÃ—k)     (dÃ—r)\n",
        "                    trainable trainable\n",
        "```\n",
        "\n",
        "1. **Freeze the original weights** $W_0$ ,  these don't receive gradients\n",
        "2. **Initialize A** with random Gaussian values\n",
        "3. **Initialize B** with zeros ,  so $BA = 0$ at the start (no change to pre-trained behavior)\n",
        "4. **Scale the output** by $\\alpha/r$ where $\\alpha$ is `lora_alpha`\n",
        "\n",
        "The scaling factor $\\alpha/r$ controls the magnitude of the LoRA update relative to the original weights.\n",
        "\n",
        "### ğŸ“Š Key LoRA Hyperparameters\n",
        "\n",
        "| Parameter | Description | Typical Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `r` (rank) | Rank of the low-rank matrices. Higher = more capacity but more parameters | 8, 16, 32, 64 |\n",
        "| `lora_alpha` | Scaling factor. The update is scaled by `alpha/r` | 16, 32 (often 2Ã— rank) |\n",
        "| `lora_dropout` | Dropout applied to LoRA layers for regularization | 0.0 - 0.1 |\n",
        "| `target_modules` | Which layers to apply LoRA to | Attention (q,k,v,o) and MLP layers |\n",
        "\n",
        "### ğŸ¯ Which Layers to Target?\n",
        "\n",
        "Research shows that applying LoRA to **attention layers** is most effective:\n",
        "- `q_proj`, `k_proj`, `v_proj`, `o_proj` ,  Query, Key, Value, and Output projections\n",
        "\n",
        "For even better results, also target **MLP layers**:\n",
        "- `gate_proj`, `up_proj`, `down_proj` ,  Feed-forward network projections\n",
        "\n",
        "### âœ… Advantages of LoRA\n",
        "\n",
        "1. **Memory efficient**: Only train ~0.1-1% of parameters\n",
        "2. **Fast training**: Fewer gradients to compute\n",
        "3. **Small adapters**: Save/share just the LoRA weights (~10-50 MB)\n",
        "4. **Composable**: Switch adapters at inference time for different tasks\n",
        "5. **No inference latency**: Merge LoRA weights into base model for deployment\n",
        "\n",
        "### âš ï¸ LoRA vs Full Fine-Tuning\n",
        "\n",
        "| Aspect | Full Fine-Tuning | LoRA |\n",
        "|--------|------------------|------|\n",
        "| Trainable params | 100% | ~0.1-1% |\n",
        "| GPU memory | Very high | Moderate |\n",
        "| Training speed | Slow | Fast |\n",
        "| Storage per task | Full model | Small adapter |\n",
        "| Quality | Best possible | Near full fine-tuning |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ–¥ï¸ Runtime Checklist\n",
        "\n",
        "1. Go to **Runtime â–¸ Change runtime type** and choose `T4` (free) or `L4/A100` (Colab Pro/Pro+).\n",
        "2. Ensure the **Hardware accelerator** is set to GPU.\n",
        "3. After connecting, run the cell below to confirm that the GPU is visible to PyTorch.\n",
        "\n",
        "> Full-precision LoRA consumes more VRAM than QLoRA. If you only have ~16 GB, prefer 7B models with modest batch sizes.\n",
        "\n",
        "### ğŸ’¾ Memory Requirements for Full-Precision LoRA\n",
        "\n",
        "| Model Size | Model Weights (bf16) | Gradients | Optimizer States | Total (Approx.) |\n",
        "|------------|---------------------|-----------|------------------|-----------------|\n",
        "| 7B | ~14 GB | ~14 GB | ~28 GB | ~56 GB |\n",
        "| **7B + LoRA** | **~14 GB** | **~0.1 GB** | **~0.2 GB** | **~15 GB** |\n",
        "\n",
        "With LoRA, we only compute gradients for the small adapter matrices, dramatically reducing memory usage!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“¦ Installing Dependencies\n",
        "\n",
        "The cell below installs the core libraries:\n",
        "- **`transformers`**: Hugging Face's library for loading and using pre-trained models\n",
        "- **`peft`**: Parameter-Efficient Fine-Tuning library (implements LoRA)\n",
        "- **`trl`**: Transformer Reinforcement Learning library (provides `SFTTrainer` for supervised fine-tuning)\n",
        "- **`datasets`**: For loading and processing training data\n",
        "- **`accelerate`**: Handles distributed training and mixed precision\n",
        "- **`wandb`**: Weights & Biases for experiment tracking (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U accelerate==0.30.1 datasets==2.19.1 evaluate==0.4.2 huggingface_hub==0.24.5 peft==0.11.1 sentencepiece==0.1.99 transformers==4.44.2 trl==0.9.4 wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=input(\"Paste your Hugging Face access token: \").strip(), add_to_git_credential=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“‚ Data Ingestion Options\n",
        "\n",
        "- **Upload `.txt`/`.json` files** to Colab and point `cfg.text_folder` to `/content/data`.\n",
        "- **Mount Google Drive** for larger corpora:\n",
        "  ```python\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  ```\n",
        "- **Reference a Hugging Face dataset** (e.g., `tatsu-lab/alpaca`) by setting `cfg.dataset_source=\"hf_dataset\"`.\n",
        "\n",
        "The helper below normalizes data into `instruction`, `response`, and optional `system` fields before training.\n",
        "\n",
        "### ğŸ“ Understanding the Training Data Format\n",
        "\n",
        "For instruction-tuning (teaching the model to follow instructions), we use a structured format:\n",
        "\n",
        "| Field | Description | Example |\n",
        "|-------|-------------|---------|\n",
        "| `instruction` | The task/question for the model | \"Summarize the following text:\" |\n",
        "| `response` | The expected output | \"The text discusses...\" |\n",
        "| `system` | (Optional) System prompt defining the model's persona | \"You are a helpful assistant.\" |\n",
        "\n",
        "### ğŸ“Š The Config Class Explained\n",
        "\n",
        "The configuration below controls all aspects of training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    project_name: str = \"lora-full-precision\"\n",
        "    base_model: str = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "    dataset_source: str = \"text_folder\"  # text_folder | hf_dataset\n",
        "    text_folder: str = \"/content/data\"\n",
        "    hf_dataset: str | None = None\n",
        "    max_samples: int | None = None\n",
        "    chunk_tokens: int = 1024\n",
        "    chunk_overlap: int = 128\n",
        "    system_prompt: str = \"You are a helpful assistant.\"\n",
        "    output_dir: str = \"/content/lora-output\"\n",
        "    wandb_project: str | None = None\n",
        "\n",
        "    micro_batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 8\n",
        "    epochs: float = 3.0\n",
        "    learning_rate: float = 1e-4\n",
        "    warmup_ratio: float = 0.03\n",
        "    weight_decay: float = 0.0\n",
        "    cutoff_len: int = 2048\n",
        "    lora_r: int = 16\n",
        "    lora_alpha: int = 32\n",
        "    lora_dropout: float = 0.05\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "print(asdict(cfg))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ” Config Parameters Deep Dive\n",
        "\n",
        "**Model & Data Settings:**\n",
        "- `base_model`: The pre-trained model to fine-tune (e.g., Llama-2-7b-chat)\n",
        "- `chunk_tokens` / `chunk_overlap`: For long documents, we split them into overlapping chunks\n",
        "\n",
        "**LoRA Hyperparameters:**\n",
        "- `lora_r=16`: The rank of the low-rank matrices. Lower = fewer parameters but less capacity\n",
        "- `lora_alpha=32`: Scaling factor. The update is multiplied by `alpha/r = 32/16 = 2`\n",
        "- `lora_dropout=0.05`: Regularization to prevent overfitting\n",
        "\n",
        "**Training Hyperparameters:**\n",
        "- `micro_batch_size=1`: Samples per GPU per step (keep low for memory)\n",
        "- `gradient_accumulation_steps=8`: Effective batch size = 1 Ã— 8 = 8\n",
        "- `learning_rate=1e-4`: LoRA typically uses higher LR than full fine-tuning\n",
        "- `cutoff_len=2048`: Maximum sequence length (longer = more memory)\n",
        "\n",
        "### ğŸ“ˆ Data Processing Pipeline\n",
        "\n",
        "The code below implements:\n",
        "1. **Text normalization**: Remove extra whitespace, standardize formatting\n",
        "2. **Chunking**: Split long documents into overlapping segments\n",
        "3. **Format conversion**: Transform various data formats into our standard schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "\n",
        "def _normalize(text: str) -> str:\n",
        "    text = text.replace(\"\\r\", \" \").strip()\n",
        "    return re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "\n",
        "def _chunk_words(text: str, chunk_tokens: int, overlap: int) -> list[str]:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    step = max(chunk_tokens - overlap, 1)\n",
        "    chunks = []\n",
        "    for start in range(0, len(words), step):\n",
        "        segment = words[start:start + chunk_tokens]\n",
        "        if len(segment) < 32:\n",
        "            continue\n",
        "        chunks.append(\" \".join(segment))\n",
        "    return chunks or [\" \".join(words[:chunk_tokens])]\n",
        "\n",
        "\n",
        "def _load_local_texts(folder: str, cfg: Config) -> list[dict]:\n",
        "    folder_path = Path(folder)\n",
        "    rows = []\n",
        "    for path in folder_path.rglob(\"*.txt\"):\n",
        "        text = path.read_text(encoding=\"utf-8\")\n",
        "        for chunk in _chunk_words(_normalize(text), cfg.chunk_tokens, cfg.chunk_overlap):\n",
        "            rows.append({\n",
        "                \"instruction\": f\"Answer using {path.stem} context.\",\n",
        "                \"response\": chunk,\n",
        "                \"system\": cfg.system_prompt,\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def _load_hf_dataset(repo_id: str, cfg: Config) -> list[dict]:\n",
        "    ds = load_dataset(repo_id, split=\"train\")\n",
        "    rows = []\n",
        "    for row in ds:\n",
        "        if {\"instruction\", \"output\"}.issubset(ds.column_names):\n",
        "            instruction = row[\"instruction\"]\n",
        "            response = row[\"output\"]\n",
        "        else:\n",
        "            instruction = row.get(\"instruction\", \"Summarize the passage:\")\n",
        "            response = row.get(\"text\", row.get(\"response\", \"\"))\n",
        "        rows.append({\n",
        "            \"instruction\": _normalize(str(instruction)),\n",
        "            \"response\": _normalize(str(response)),\n",
        "            \"system\": row.get(\"system\", cfg.system_prompt),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def build_dataset(cfg: Config) -> Dataset:\n",
        "    if cfg.dataset_source == \"hf_dataset\" and cfg.hf_dataset:\n",
        "        rows = _load_hf_dataset(cfg.hf_dataset, cfg)\n",
        "    else:\n",
        "        rows = _load_local_texts(cfg.text_folder, cfg)\n",
        "\n",
        "    if cfg.max_samples:\n",
        "        random.seed(cfg.seed)\n",
        "        rows = random.sample(rows, min(cfg.max_samples, len(rows)))\n",
        "\n",
        "    rows = [r for r in rows if r[\"instruction\"].strip() and r[\"response\"].strip()]\n",
        "    dataset = Dataset.from_pandas(pd.DataFrame(rows))\n",
        "    print(f\"Dataset has {len(dataset)} rows after cleaning\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = build_dataset(cfg)\n",
        "dataset[:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ’¬ Prompt Templating\n",
        "\n",
        "We build chat prompts using the tokenizer's `chat_template` when available so that LoRA adapters stay aligned with the base instruction format.\n",
        "\n",
        "### ğŸ¯ Why Prompt Templates Matter\n",
        "\n",
        "Different models expect different prompt formats. Using the wrong format leads to poor results!\n",
        "\n",
        "**Llama-2 Chat format:**\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "You are a helpful assistant.\n",
        "<</SYS>>\n",
        "\n",
        "What is the capital of France? [/INST] The capital of France is Paris. </s>\n",
        "```\n",
        "\n",
        "**Mistral Instruct format:**\n",
        "```\n",
        "<s>[INST] What is the capital of France? [/INST] The capital of France is Paris.</s>\n",
        "```\n",
        "\n",
        "The `chat_template` in the tokenizer automatically handles these differences, ensuring your fine-tuned model uses the correct format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=False)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def build_prompt(example: dict) -> dict:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": example.get(\"system\") or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
        "    ]\n",
        "    if tokenizer.chat_template:\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"[SYSTEM]\\n{messages[0]['content']}\\n\\n\"\n",
        "            f\"[USER]\\n{messages[1]['content']}\\n\\n\"\n",
        "            f\"[ASSISTANT]\\n{messages[2]['content']}\"\n",
        "        )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "\n",
        "processed_dataset = dataset.map(build_prompt, remove_columns=dataset.column_names)\n",
        "processed_dataset = processed_dataset.shuffle(seed=cfg.seed)\n",
        "splits = processed_dataset.train_test_split(test_size=0.05, seed=cfg.seed)\n",
        "splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸš€ LoRA Training\n",
        "\n",
        "Without quantization, we load the base model in bf16/fp16, attach LoRA adapters to attention/MLP modules, and fine-tune via `trl.SFTTrainer`. Keep `micro_batch_size` low to stay within VRAM limits.\n",
        "\n",
        "### ğŸ”§ Training Pipeline Breakdown\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        TRAINING FLOW                            â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  1. Load Base Model (bf16)                                      â”‚\n",
        "â”‚     â””â”€â–º 7B params loaded, ~14GB VRAM                           â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  2. Enable Gradient Checkpointing                               â”‚\n",
        "â”‚     â””â”€â–º Trade compute for memory (recompute activations)       â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  3. Attach LoRA Adapters                                        â”‚\n",
        "â”‚     â””â”€â–º Add small trainable matrices to target layers          â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  4. Freeze Base Weights                                         â”‚\n",
        "â”‚     â””â”€â–º Only LoRA parameters receive gradients                 â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚  5. Train with SFTTrainer                                       â”‚\n",
        "â”‚     â””â”€â–º Supervised Fine-Tuning on instruction-response pairs   â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“‹ Key Components Explained\n",
        "\n",
        "**`LoraConfig` Parameters:**\n",
        "```python\n",
        "LoraConfig(\n",
        "    r=16,                    # Rank: size of low-rank matrices\n",
        "    lora_alpha=32,           # Scaling: output multiplied by alpha/r\n",
        "    target_modules=[...],    # Which layers to adapt\n",
        "    lora_dropout=0.05,       # Regularization\n",
        "    bias=\"none\",             # Don't train bias terms\n",
        "    task_type=\"CAUSAL_LM\",   # Language modeling task\n",
        ")\n",
        "```\n",
        "\n",
        "**`SFTTrainer` Features:**\n",
        "- Handles tokenization and padding automatically\n",
        "- `packing=True`: Concatenates short sequences to fill context length (more efficient)\n",
        "- Computes loss only on assistant responses (not system/user prompts)\n",
        "\n",
        "### âš¡ Memory Optimization Techniques Used\n",
        "\n",
        "1. **Gradient Checkpointing**: Recomputes activations during backward pass instead of storing them\n",
        "2. **Mixed Precision (bf16)**: Uses 16-bit floats for most operations\n",
        "3. **LoRA**: Only stores gradients for ~0.1% of parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "if cfg.wandb_project:\n",
        "    os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project\n",
        "else:\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.base_model,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=cfg.lora_r,\n",
        "    lora_alpha=cfg.lora_alpha,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=cfg.lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=cfg.output_dir,\n",
        "    per_device_train_batch_size=cfg.micro_batch_size,\n",
        "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
        "    num_train_epochs=cfg.epochs,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    weight_decay=cfg.weight_decay,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    bf16=torch_dtype == torch.bfloat16,\n",
        "    fp16=torch_dtype == torch.float16,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=([] if os.environ.get(\"WANDB_DISABLED\") == \"true\" else [\"wandb\"]),\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=splits[\"train\"],\n",
        "    eval_dataset=splits[\"test\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=cfg.cutoff_len,\n",
        "    packing=True,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(cfg.output_dir)\n",
        "tokenizer.save_pretrained(cfg.output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Evaluating the Model: Perplexity\n",
        "\n",
        "**Perplexity** measures how \"surprised\" the model is by the test data. Lower is better!\n",
        "\n",
        "$$\\text{Perplexity} = e^{\\text{Cross-Entropy Loss}}$$\n",
        "\n",
        "**Interpretation:**\n",
        "- Perplexity of 10 means the model is \"as confused as if it had to choose uniformly between 10 options\"\n",
        "- A well-trained model on domain-specific data typically achieves perplexity of 5-20\n",
        "- Base models on random text: perplexity 20-100+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def compute_perplexity(eval_dataset, max_batches: int = 32) -> float:\n",
        "    model.eval()\n",
        "    loader = DataLoader(eval_dataset[\"text\"], batch_size=1)\n",
        "    scores = []\n",
        "    for idx, batch in enumerate(loader):\n",
        "        if idx >= max_batches:\n",
        "            break\n",
        "        encoded = tokenizer(batch[0], return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded, labels=encoded[\"input_ids\"])\n",
        "        scores.append(math.exp(outputs.loss.item()))\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "perplexity = compute_perplexity(splits[\"test\"], max_batches=32)\n",
        "print(f\"Approximate perplexity: {perplexity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ—£ï¸ Interactive Testing\n",
        "\n",
        "The `chat()` function allows you to test your fine-tuned model interactively:\n",
        "- Uses the same prompt template as training for consistency\n",
        "- `do_sample=True` enables creative/varied responses\n",
        "- `top_p=0.9` (nucleus sampling) and `temperature=0.7` control randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(prompt: str, system: str | None = None, max_new_tokens: int = 512) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(template, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    output_tokens = generated[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "chat(\"Summarize the three biggest takeaways from our training data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ”€ Merging LoRA Adapters\n",
        "\n",
        "After training, you have two deployment options:\n",
        "\n",
        "**Option 1: Keep Adapters Separate**\n",
        "- Load base model + LoRA adapter at inference\n",
        "- Allows hot-swapping different adapters\n",
        "- Slight overhead for loading adapters\n",
        "\n",
        "**Option 2: Merge and Unload (shown below)**\n",
        "- Permanently bake LoRA weights into the base model: $W_{merged} = W_0 + BA$\n",
        "- Results in a standard model (no PEFT dependency needed)\n",
        "- No runtime overhead\n",
        "- Larger storage (full model size)\n",
        "\n",
        "```\n",
        "Before Merge:                    After Merge:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Base Wâ‚€ â”‚ + â”‚ LoRA BA â”‚  ==>  â”‚ Wâ‚€ + BA (merged)â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "  ~14 GB        ~50 MB                ~14 GB\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "MERGED_DIR = Path(cfg.output_dir) / \"merged\"\n",
        "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "merged_model = AutoPeftModelForCausalLM.from_pretrained(cfg.output_dir, device_map=\"auto\")\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "merged_model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "print(\"Merged checkpoint saved to\", MERGED_DIR)\n",
        "\n",
        "push_to_hub = False\n",
        "if push_to_hub:\n",
        "    repo_id = input(\"Target HF repo (username/project): \")\n",
        "    merged_model.push_to_hub(repo_id, private=True)\n",
        "    tokenizer.push_to_hub(repo_id, private=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Next Steps & Best Practices\n",
        "\n",
        "### ğŸ“ Workflow Recommendations\n",
        "- Adjust the config block (`cfg`) for your dataset, model, and hyperparameters before running\n",
        "- Monitor GPU memory via `nvidia-smi` while training; reduce `micro_batch_size` or `cutoff_len` if you hit OOM\n",
        "- Evaluate the resulting adapter on task-specific prompts and log findings (W&B, TensorBoard, etc.)\n",
        "\n",
        "### ğŸš€ Deployment Options\n",
        "\n",
        "| Option | Description | When to Use |\n",
        "|--------|-------------|-------------|\n",
        "| **Separate adapter** | Load base + `PeftModel.from_pretrained` | Multi-task, adapter swapping |\n",
        "| **Merged checkpoint** | Single model, no PEFT dependency | Simpler deployment |\n",
        "| **vLLM/TGI** | Production serving with LoRA support | High-throughput APIs |\n",
        "\n",
        "### ğŸ”§ Troubleshooting Common Issues\n",
        "\n",
        "| Issue | Solution |\n",
        "|-------|----------|\n",
        "| OOM during training | Reduce `micro_batch_size`, enable gradient checkpointing |\n",
        "| Loss plateaus | Try higher `learning_rate` or larger `lora_r` |\n",
        "| Overfitting | Increase `lora_dropout`, reduce epochs, add more data |\n",
        "| Poor generation | Verify prompt template matches training format |\n",
        "\n",
        "### ğŸ“Š LoRA vs QLoRA: When to Use Which?\n",
        "\n",
        "| Choose **LoRA** when: | Choose **QLoRA** when: |\n",
        "|-----------------------|------------------------|\n",
        "| You have â‰¥24GB VRAM | Limited GPU memory (<16GB) |\n",
        "| Maximum quality matters | Memory efficiency is priority |\n",
        "| Faster training needed | Training larger models (13B+) |\n",
        "| Simpler setup preferred | Free Colab T4 runtime |\n",
        "\n",
        "### ğŸ“š Further Reading\n",
        "- [LoRA Paper](https://arxiv.org/abs/2106.09685) - Original research by Microsoft\n",
        "- [PEFT Documentation](https://huggingface.co/docs/peft) - Hugging Face implementation\n",
        "- [TRL Documentation](https://huggingface.co/docs/trl) - Training library details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“– Summary: Key Concepts\n",
        "\n",
        "### ğŸ§  LoRA Essentials\n",
        "\n",
        "1. **Low-Rank Decomposition**: Instead of updating $W$ directly, decompose the update as $\\Delta W = BA$ where $B$ and $A$ are small matrices\n",
        "2. **Parameter Efficiency**: Train only 0.1-1% of parameters while achieving near full fine-tuning quality\n",
        "3. **Memory Savings**: No gradients for frozen base weights = massive memory reduction\n",
        "4. **Composability**: Multiple LoRA adapters can be trained and swapped at inference time\n",
        "\n",
        "### ğŸ”¢ The Math That Matters\n",
        "\n",
        "$$W_{new} = W_{frozen} + \\frac{\\alpha}{r} \\cdot B \\cdot A$$\n",
        "\n",
        "- $r$ (rank): Controls adapter capacity\n",
        "- $\\alpha$: Controls the magnitude of the update\n",
        "- $\\alpha/r$: The effective scaling factor\n",
        "\n",
        "### âœ… Quick Reference\n",
        "\n",
        "| Task | Recommended `r` | Notes |\n",
        "|------|-----------------|-------|\n",
        "| Simple adaptation | 8-16 | Fast, lightweight |\n",
        "| Complex tasks | 32-64 | More capacity |\n",
        "| Full performance | 128+ | Approaches full fine-tuning |\n",
        "\n",
        "---\n",
        "*Happy fine-tuning! ğŸš€*"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
