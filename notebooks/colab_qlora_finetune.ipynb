{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab QLoRA Fine-Tuning Tool\n",
        "\n",
        "This notebook walks you through training instruction-following adapters (LoRA) on top of open-source chat models such as `meta-llama/Llama-2-7b-chat-hf` using the QLoRA technique. Each section explains **what** to run and **why it matters** so you can confidently adapt the workflow to new datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Runtime checklist\n",
        "\n",
        "1. Go to **Runtime ▸ Change runtime type** and pick `T4 GPU` (L4/A100 if you have Colab Pro/Pro+).\n",
        "2. Toggle **GPU** and keep the rest default.\n",
        "3. Connect the runtime before running the cells below.\n",
        "\n",
        "> **Why:** QLoRA loads the base model in 4-bit precision, so a T4's 16 GB VRAM is sufficient for 7B chat models when you keep batch sizes modest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U accelerate==0.27.2 bitsandbytes==0.43.0 datasets==2.17.0 evaluate==0.4.1 huggingface_hub==0.21.4 peft==0.8.2 sentencepiece==0.1.99 transformers==4.38.2 trl==0.7.10 wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=input(\"Paste your Hugging Face access token: \").strip(), add_to_git_credential=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data ingestion options\n",
        "\n",
        "- **Upload text files** (left sidebar ▸ Files ▸ Upload) and set `DATASET_SOURCE=\"text_folder\"`.\n",
        "- **Mount Google Drive** for larger corpora:\n",
        "  ```python\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  ```\n",
        "- **Use a public Hugging Face dataset** by supplying its repo id (e.g., `tatsu-lab/alpaca`).\n",
        "\n",
        "The helper below consolidates these flows and creates a `datasets.Dataset` with `instruction`, `response`, and optional `system` fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    project_name: str = \"qlora-custom-data\"\n",
        "    base_model: str = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "    dataset_source: str = \"text_folder\"  # text_folder | hf_dataset\n",
        "    text_folder: str = \"/content/data\"   # ignored when using hf_dataset\n",
        "    hf_dataset: str | None = None         # e.g. \"tatsu-lab/alpaca\"\n",
        "    max_samples: int | None = None\n",
        "    chunk_tokens: int = 1024\n",
        "    chunk_overlap: int = 128\n",
        "    system_prompt: str = \"You are a helpful assistant that responds with concise, domain-specific answers.\"\n",
        "    output_dir: str = \"/content/qlora-output\"\n",
        "    wandb_project: str | None = None\n",
        "\n",
        "    # Training hyperparams\n",
        "    micro_batch_size: int = 4\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    epochs: float = 3.0\n",
        "    learning_rate: float = 2e-4\n",
        "    warmup_ratio: float = 0.03\n",
        "    weight_decay: float = 0.0\n",
        "    cutoff_len: int = 2048\n",
        "    lora_r: int = 64\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.1\n",
        "    seed: int = 42\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "print(asdict(cfg))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "\n",
        "def _normalize(text: str) -> str:\n",
        "    text = text.replace(\"\\r\", \" \").strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def _chunk_words(text: str, chunk_tokens: int, overlap: int) -> list[str]:\n",
        "    words = text.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    step = max(chunk_tokens - overlap, 1)\n",
        "    chunks = []\n",
        "    for start in range(0, len(words), step):\n",
        "        segment = words[start:start + chunk_tokens]\n",
        "        if len(segment) < 32:\n",
        "            continue\n",
        "        chunks.append(\" \".join(segment))\n",
        "    return chunks or [\" \".join(words[:chunk_tokens])]\n",
        "\n",
        "\n",
        "def _load_local_texts(folder: str, cfg: Config) -> list[dict]:\n",
        "    folder_path = Path(folder)\n",
        "    rows = []\n",
        "    for path in folder_path.rglob(\"*.txt\"):\n",
        "        text = path.read_text(encoding=\"utf-8\")\n",
        "        for chunk in _chunk_words(_normalize(text), cfg.chunk_tokens, cfg.chunk_overlap):\n",
        "            rows.append({\n",
        "                \"instruction\": f\"Answer the following based on {path.stem}:\",\n",
        "                \"response\": chunk,\n",
        "                \"system\": cfg.system_prompt,\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def _load_hf_dataset(repo_id: str, cfg: Config) -> list[dict]:\n",
        "    ds = load_dataset(repo_id, split=\"train\")\n",
        "    fields = set(ds.column_names)\n",
        "    rows = []\n",
        "    for row in ds:\n",
        "        if {\"instruction\", \"output\"}.issubset(fields):\n",
        "            response = row[\"output\"]\n",
        "            instruction = row[\"instruction\"]\n",
        "        else:\n",
        "            # Fallback to single text field\n",
        "            response = row.get(\"text\", row.get(\"response\", \"\"))\n",
        "            instruction = row.get(\"instruction\", \"Summarize the passage:\")\n",
        "        rows.append({\n",
        "            \"instruction\": _normalize(str(instruction)),\n",
        "            \"response\": _normalize(str(response)),\n",
        "            \"system\": row.get(\"system\", cfg.system_prompt),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def build_dataset(cfg: Config) -> Dataset:\n",
        "    if cfg.dataset_source == \"hf_dataset\" and cfg.hf_dataset:\n",
        "        rows = _load_hf_dataset(cfg.hf_dataset, cfg)\n",
        "    else:\n",
        "        rows = _load_local_texts(cfg.text_folder, cfg)\n",
        "\n",
        "    if cfg.max_samples:\n",
        "        random.seed(cfg.seed)\n",
        "        rows = random.sample(rows, min(cfg.max_samples, len(rows)))\n",
        "\n",
        "    clean_rows = [r for r in rows if r[\"instruction\"].strip() and r[\"response\"].strip()]\n",
        "    dataset = Dataset.from_pandas(pd.DataFrame(clean_rows))\n",
        "    print(f\"Dataset has {len(dataset)} rows after cleaning\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = build_dataset(cfg)\n",
        "dataset[:2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt templating\n",
        "\n",
        "We rely on the tokenizer's built-in chat template (when available) so that formatting always matches the base model's expectations. If the checkpoint lacks a template, we fall back to a simple system/instruction/response triple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=False)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def build_prompt(example: dict) -> dict:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": example.get(\"system\") or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
        "    ]\n",
        "    if tokenizer.chat_template:\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"[SYSTEM]\\n{messages[0]['content']}\\n\\n\"\n",
        "            f\"[USER]\\n{messages[1]['content']}\\n\\n\"\n",
        "            f\"[ASSISTANT]\\n{messages[2]['content']}\"\n",
        "        )\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "\n",
        "processed_dataset = dataset.map(build_prompt, remove_columns=dataset.column_names)\n",
        "processed_dataset = processed_dataset.shuffle(seed=cfg.seed)\n",
        "splits = processed_dataset.train_test_split(test_size=0.05, seed=cfg.seed)\n",
        "splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QLoRA training\n",
        "\n",
        "We load the base model in 4-bit NF4 using `BitsAndBytesConfig`, attach LoRA adapters to attention + MLP modules, and fine-tune with `trl.SFTTrainer` so padding/truncation are handled automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "if cfg.wandb_project:\n",
        "    os.environ[\"WANDB_PROJECT\"] = cfg.wandb_project\n",
        "else:\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=cfg.lora_r,\n",
        "    lora_alpha=cfg.lora_alpha,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=cfg.lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=cfg.output_dir,\n",
        "    per_device_train_batch_size=cfg.micro_batch_size,\n",
        "    gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
        "    num_train_epochs=cfg.epochs,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    bf16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    weight_decay=cfg.weight_decay,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=([] if os.environ.get(\"WANDB_DISABLED\") == \"true\" else [\"wandb\"]),\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=splits[\"train\"],\n",
        "    eval_dataset=splits[\"test\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=cfg.cutoff_len,\n",
        "    packing=True,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(cfg.output_dir)\n",
        "tokenizer.save_pretrained(cfg.output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def compute_perplexity(eval_dataset, max_batches: int = 32) -> float:\n",
        "    model.eval()\n",
        "    ppl_scores = []\n",
        "    loader = DataLoader(eval_dataset[\"text\"], batch_size=1)\n",
        "    for idx, batch in enumerate(loader):\n",
        "        if idx >= max_batches:\n",
        "            break\n",
        "        encoded = tokenizer(batch[0], return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded, labels=encoded[\"input_ids\"])\n",
        "        ppl_scores.append(math.exp(outputs.loss.item()))\n",
        "    return sum(ppl_scores) / len(ppl_scores)\n",
        "\n",
        "\n",
        "perplexity = compute_perplexity(splits[\"test\"], max_batches=32)\n",
        "print(f\"Approximate perplexity: {perplexity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(prompt: str, system: str | None = None, max_new_tokens: int = 512) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system or cfg.system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(template, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    output = generated[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "chat(\"Summarize the three most important facts from our dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "\n",
        "MERGED_DIR = Path(cfg.output_dir) / \"merged\"\n",
        "MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "merged_model = AutoPeftModelForCausalLM.from_pretrained(cfg.output_dir, device_map=\"auto\")\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "merged_model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "print(\"Merged checkpoint saved to\", MERGED_DIR)\n",
        "\n",
        "upload_to_hub = False  # flip to True to push\n",
        "if upload_to_hub:\n",
        "    repo_id = input(\"Target HF repo (e.g. username/project-name): \")\n",
        "    merged_model.push_to_hub(repo_id, private=True)\n",
        "    tokenizer.push_to_hub(repo_id, private=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Duplicate this notebook per project so you can version-control configs.\n",
        "- Replace the evaluation prompts with task-specific checklists (safety, compliance, tone).\n",
        "- Deploy LoRA adapters with frameworks like Text Generation Inference (TGI) or vLLM by loading the base model and calling `PeftModel.from_pretrained`. For serverless workflows, merge the adapters (cell above) and upload the full checkpoint.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
